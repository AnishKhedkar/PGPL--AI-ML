{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QGIsF1ADyJ58"
   },
   "source": [
    "# Transfer Learning CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-n6tVFayGBe"
   },
   "source": [
    "* Train a simple convnet on the CIFAR dataset the first 5 output classes [0..4].\n",
    "* Freeze convolutional layers and fine-tune dense layers for the last 5 ouput classes [5..9].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cq8ejXHJyGYq"
   },
   "source": [
    "### 1. Import CIFAR10 data and create 2 datasets with one dataset having classes from 0 to 4 and other having classes from 5 to 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uWYbxnBayFUP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cifar10 = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_df), (test_df) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_04 = []\n",
    "x_train_59 = []\n",
    "y_train_04 = []\n",
    "y_train_59 = []\n",
    "\n",
    "for idx, item in enumerate(train_df[1]):\n",
    "    if item[0] in [0,1,2,3,4]:\n",
    "        x_train_04.append(train_df[0][idx])\n",
    "        y_train_04.append(item[0])\n",
    "    elif item[0] in [5,6,7,8,9]:\n",
    "        x_train_59.append(train_df[0][idx])\n",
    "        y_train_59.append(item[0])\n",
    "x_train_04 = np.array(x_train_04)\n",
    "x_train_59 = np.array(x_train_59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_04 = []\n",
    "x_test_59 = []\n",
    "y_test_04 = []\n",
    "y_test_59 = []\n",
    "\n",
    "for idx, item in enumerate(test_df[1]):\n",
    "    if item[0] in [0,1,2,3,4]:\n",
    "        x_test_04.append(test_df[0][idx])\n",
    "        y_test_04.append(item[0])\n",
    "    elif item[0] in [5,6,7,8,9]:\n",
    "        x_test_59.append(test_df[0][idx])\n",
    "        y_test_59.append(item[0])\n",
    "x_test_04 = np.array(x_test_04)\n",
    "x_test_59 = np.array(x_test_59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xtCKmQh4yXhT"
   },
   "source": [
    "### 2. Use One-hot encoding to divide y_train and y_test into required no of output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uN5O2kJ3yYa6"
   },
   "outputs": [],
   "source": [
    "y_train_04_one_hot = pd.get_dummies(np.array(y_train_04))\n",
    "y_train_59_one_hot = pd.get_dummies(np.array(y_train_59))\n",
    "y_test_04_one_hot = pd.get_dummies(np.array(y_test_04))\n",
    "y_test_59_one_hot = pd.get_dummies(np.array(y_test_59))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4\n",
       "0  0  0  0  0  1\n",
       "1  0  1  0  0  0\n",
       "2  0  1  0  0  0\n",
       "3  0  0  1  0  0\n",
       "4  0  0  0  1  0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_04_one_hot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuOiKWfeybAl"
   },
   "source": [
    "### 3. Build a sequential neural network model which can classify the classes 0 to 4 of CIFAR10 dataset with at least 80% accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_04.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5HzxNbiiyoBD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mac/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/mac/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 13, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 37,605\n",
      "Trainable params: 37,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape= x_train_04.shape[1:]))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_04_scratch = x_train_04/255\n",
    "X_test_04_scratch = x_test_04/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 32, 32, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_04_scratch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_04_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24970</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24971</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24972</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24973</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24974</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24975</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24976</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24977</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24978</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24979</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24980</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24981</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24982</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24983</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24984</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24985</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24986</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24987</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24988</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24989</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24991</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24992</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24993</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24994</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2  3  4\n",
       "0      0  0  0  0  1\n",
       "1      0  1  0  0  0\n",
       "2      0  1  0  0  0\n",
       "3      0  0  1  0  0\n",
       "4      0  0  0  1  0\n",
       "5      0  0  0  0  1\n",
       "6      0  0  1  0  0\n",
       "7      0  0  0  1  0\n",
       "8      0  0  1  0  0\n",
       "9      0  0  0  0  1\n",
       "10     0  0  0  1  0\n",
       "11     0  0  1  0  0\n",
       "12     0  0  0  1  0\n",
       "13     0  0  0  0  1\n",
       "14     1  0  0  0  0\n",
       "15     1  0  0  0  0\n",
       "16     0  1  0  0  0\n",
       "17     0  0  0  1  0\n",
       "18     0  0  0  0  1\n",
       "19     1  0  0  0  0\n",
       "20     0  0  0  1  0\n",
       "21     0  0  0  1  0\n",
       "22     0  0  0  1  0\n",
       "23     0  0  1  0  0\n",
       "24     0  0  1  0  0\n",
       "25     0  1  0  0  0\n",
       "26     0  1  0  0  0\n",
       "27     0  1  0  0  0\n",
       "28     0  0  1  0  0\n",
       "29     0  0  1  0  0\n",
       "...   .. .. .. .. ..\n",
       "24970  0  0  0  0  1\n",
       "24971  0  0  0  0  1\n",
       "24972  0  0  0  0  1\n",
       "24973  0  0  0  0  1\n",
       "24974  0  0  0  0  1\n",
       "24975  0  0  1  0  0\n",
       "24976  0  0  0  1  0\n",
       "24977  0  1  0  0  0\n",
       "24978  0  0  1  0  0\n",
       "24979  0  0  0  1  0\n",
       "24980  0  0  1  0  0\n",
       "24981  0  1  0  0  0\n",
       "24982  0  0  0  0  1\n",
       "24983  0  0  0  1  0\n",
       "24984  0  1  0  0  0\n",
       "24985  0  0  0  1  0\n",
       "24986  0  0  0  0  1\n",
       "24987  0  0  0  1  0\n",
       "24988  0  0  0  1  0\n",
       "24989  0  0  0  0  1\n",
       "24990  0  0  1  0  0\n",
       "24991  0  1  0  0  0\n",
       "24992  0  0  0  0  1\n",
       "24993  0  0  1  0  0\n",
       "24994  1  0  0  0  0\n",
       "24995  0  1  0  0  0\n",
       "24996  1  0  0  0  0\n",
       "24997  0  0  1  0  0\n",
       "24998  0  1  0  0  0\n",
       "24999  0  1  0  0  0\n",
       "\n",
       "[25000 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_04_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mac/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "20000/20000 [==============================] - 11s 563us/step - loss: 1.2531 - acc: 0.4357 - val_loss: 1.0622 - val_acc: 0.5590\n",
      "Epoch 2/50\n",
      "20000/20000 [==============================] - 12s 601us/step - loss: 1.0363 - acc: 0.5672 - val_loss: 0.9432 - val_acc: 0.6258\n",
      "Epoch 3/50\n",
      "20000/20000 [==============================] - 12s 608us/step - loss: 0.9517 - acc: 0.6137 - val_loss: 0.8908 - val_acc: 0.6482\n",
      "Epoch 4/50\n",
      "20000/20000 [==============================] - 12s 601us/step - loss: 0.8901 - acc: 0.6439 - val_loss: 0.8314 - val_acc: 0.6616\n",
      "Epoch 5/50\n",
      "20000/20000 [==============================] - 12s 611us/step - loss: 0.8357 - acc: 0.6713 - val_loss: 0.7891 - val_acc: 0.6940\n",
      "Epoch 6/50\n",
      "20000/20000 [==============================] - 12s 618us/step - loss: 0.7964 - acc: 0.6880 - val_loss: 0.7305 - val_acc: 0.7234\n",
      "Epoch 7/50\n",
      "20000/20000 [==============================] - 13s 650us/step - loss: 0.7523 - acc: 0.7106 - val_loss: 0.7493 - val_acc: 0.7222\n",
      "Epoch 8/50\n",
      "20000/20000 [==============================] - 13s 625us/step - loss: 0.7293 - acc: 0.7196 - val_loss: 0.7225 - val_acc: 0.7278\n",
      "Epoch 9/50\n",
      "20000/20000 [==============================] - 13s 627us/step - loss: 0.7039 - acc: 0.7323 - val_loss: 0.6602 - val_acc: 0.7528\n",
      "Epoch 10/50\n",
      "20000/20000 [==============================] - 13s 625us/step - loss: 0.6883 - acc: 0.7379 - val_loss: 0.6500 - val_acc: 0.7576\n",
      "Epoch 11/50\n",
      "20000/20000 [==============================] - 13s 629us/step - loss: 0.6642 - acc: 0.7463 - val_loss: 0.6185 - val_acc: 0.7628\n",
      "Epoch 12/50\n",
      "20000/20000 [==============================] - 13s 632us/step - loss: 0.6475 - acc: 0.7528 - val_loss: 0.6132 - val_acc: 0.7710\n",
      "Epoch 13/50\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.6344 - acc: 0.7571 - val_loss: 0.6023 - val_acc: 0.7692\n",
      "Epoch 14/50\n",
      "20000/20000 [==============================] - 13s 634us/step - loss: 0.6265 - acc: 0.7615 - val_loss: 0.5950 - val_acc: 0.7760\n",
      "Epoch 15/50\n",
      "20000/20000 [==============================] - 13s 659us/step - loss: 0.6030 - acc: 0.7699 - val_loss: 0.5874 - val_acc: 0.7852\n",
      "Epoch 16/50\n",
      "20000/20000 [==============================] - 13s 634us/step - loss: 0.5940 - acc: 0.7758 - val_loss: 0.5943 - val_acc: 0.7816\n",
      "Epoch 17/50\n",
      "20000/20000 [==============================] - 13s 634us/step - loss: 0.5819 - acc: 0.7779 - val_loss: 0.5622 - val_acc: 0.7894\n",
      "Epoch 18/50\n",
      "20000/20000 [==============================] - 13s 630us/step - loss: 0.5746 - acc: 0.7833 - val_loss: 0.6147 - val_acc: 0.7672\n",
      "Epoch 19/50\n",
      "20000/20000 [==============================] - 13s 636us/step - loss: 0.5680 - acc: 0.7867 - val_loss: 0.5762 - val_acc: 0.7834\n",
      "Epoch 20/50\n",
      "20000/20000 [==============================] - 13s 642us/step - loss: 0.5533 - acc: 0.7908 - val_loss: 0.5396 - val_acc: 0.7990\n",
      "Epoch 21/50\n",
      "20000/20000 [==============================] - 13s 629us/step - loss: 0.5519 - acc: 0.7890 - val_loss: 0.5584 - val_acc: 0.7936\n",
      "Epoch 22/50\n",
      "20000/20000 [==============================] - 13s 636us/step - loss: 0.5402 - acc: 0.7948 - val_loss: 0.5604 - val_acc: 0.7916\n",
      "Epoch 23/50\n",
      "20000/20000 [==============================] - 13s 638us/step - loss: 0.5311 - acc: 0.7984 - val_loss: 0.5376 - val_acc: 0.8058\n",
      "Epoch 24/50\n",
      "20000/20000 [==============================] - 13s 634us/step - loss: 0.5278 - acc: 0.7991 - val_loss: 0.5477 - val_acc: 0.7984\n",
      "Epoch 25/50\n",
      "20000/20000 [==============================] - 13s 633us/step - loss: 0.5149 - acc: 0.8042 - val_loss: 0.5363 - val_acc: 0.7996\n",
      "Epoch 26/50\n",
      "20000/20000 [==============================] - 13s 634us/step - loss: 0.5129 - acc: 0.8057 - val_loss: 0.5297 - val_acc: 0.8006\n",
      "Epoch 27/50\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.5030 - acc: 0.8099 - val_loss: 0.5427 - val_acc: 0.8028\n",
      "Epoch 28/50\n",
      "20000/20000 [==============================] - 13s 640us/step - loss: 0.4954 - acc: 0.8135 - val_loss: 0.5402 - val_acc: 0.7992\n",
      "Epoch 29/50\n",
      "20000/20000 [==============================] - 13s 637us/step - loss: 0.4917 - acc: 0.8168 - val_loss: 0.5202 - val_acc: 0.8100\n",
      "Epoch 30/50\n",
      "20000/20000 [==============================] - 13s 654us/step - loss: 0.4919 - acc: 0.8167 - val_loss: 0.5171 - val_acc: 0.8096\n",
      "Epoch 31/50\n",
      "20000/20000 [==============================] - 13s 636us/step - loss: 0.4838 - acc: 0.8170 - val_loss: 0.5143 - val_acc: 0.8082\n",
      "Epoch 32/50\n",
      "20000/20000 [==============================] - 13s 638us/step - loss: 0.4809 - acc: 0.8194 - val_loss: 0.5242 - val_acc: 0.8060\n",
      "Epoch 33/50\n",
      "20000/20000 [==============================] - 13s 642us/step - loss: 0.4749 - acc: 0.8206 - val_loss: 0.5225 - val_acc: 0.8106\n",
      "Epoch 34/50\n",
      "20000/20000 [==============================] - 13s 642us/step - loss: 0.4771 - acc: 0.8185 - val_loss: 0.5141 - val_acc: 0.8110\n",
      "Epoch 35/50\n",
      "20000/20000 [==============================] - 13s 642us/step - loss: 0.4658 - acc: 0.8232 - val_loss: 0.5327 - val_acc: 0.8010\n",
      "Epoch 36/50\n",
      "20000/20000 [==============================] - 13s 652us/step - loss: 0.4578 - acc: 0.8261 - val_loss: 0.5203 - val_acc: 0.8078\n",
      "Epoch 37/50\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.4564 - acc: 0.8269 - val_loss: 0.5244 - val_acc: 0.8114\n",
      "Epoch 38/50\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.4596 - acc: 0.8282 - val_loss: 0.5344 - val_acc: 0.8032\n",
      "Epoch 39/50\n",
      "20000/20000 [==============================] - 14s 679us/step - loss: 0.4484 - acc: 0.8314 - val_loss: 0.5168 - val_acc: 0.8122\n",
      "Epoch 40/50\n",
      "20000/20000 [==============================] - 14s 688us/step - loss: 0.4402 - acc: 0.8357 - val_loss: 0.4987 - val_acc: 0.8194\n",
      "Epoch 41/50\n",
      "20000/20000 [==============================] - 13s 641us/step - loss: 0.4361 - acc: 0.8356 - val_loss: 0.5180 - val_acc: 0.8028\n",
      "Epoch 42/50\n",
      "20000/20000 [==============================] - 13s 651us/step - loss: 0.4464 - acc: 0.8298 - val_loss: 0.5324 - val_acc: 0.7992\n",
      "Epoch 43/50\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.4311 - acc: 0.8333 - val_loss: 0.5206 - val_acc: 0.8160\n",
      "Epoch 44/50\n",
      "20000/20000 [==============================] - 13s 660us/step - loss: 0.4300 - acc: 0.8382 - val_loss: 0.5237 - val_acc: 0.8094\n",
      "Epoch 45/50\n",
      "20000/20000 [==============================] - 13s 654us/step - loss: 0.4248 - acc: 0.8377 - val_loss: 0.5080 - val_acc: 0.8178\n",
      "Epoch 46/50\n",
      "20000/20000 [==============================] - 13s 658us/step - loss: 0.4273 - acc: 0.8375 - val_loss: 0.5042 - val_acc: 0.8202\n",
      "Epoch 47/50\n",
      "20000/20000 [==============================] - 13s 657us/step - loss: 0.4195 - acc: 0.8414 - val_loss: 0.5029 - val_acc: 0.8172\n",
      "Epoch 48/50\n",
      "20000/20000 [==============================] - 13s 648us/step - loss: 0.4263 - acc: 0.8412 - val_loss: 0.5220 - val_acc: 0.8118\n",
      "Epoch 49/50\n",
      "20000/20000 [==============================] - 13s 664us/step - loss: 0.4196 - acc: 0.8383 - val_loss: 0.5127 - val_acc: 0.8142\n",
      "Epoch 50/50\n",
      "20000/20000 [==============================] - 13s 656us/step - loss: 0.4073 - acc: 0.8465 - val_loss: 0.5004 - val_acc: 0.8200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb275c5940>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the model on the train data and labels.\n",
    "model.fit(X_train_04_scratch, y_train_04_one_hot, batch_size=32, epochs=50, \n",
    "          verbose=1, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 162us/step\n",
      "Accuracy on the Test Images:  0.8068\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model on the test data\n",
    "score = model.evaluate(X_test_04_scratch, y_test_04_one_hot)\n",
    "\n",
    "#Accuracy on test data\n",
    "print('Accuracy on the Test Images: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_59_scratch = x_train_59/255\n",
    "X_test_59_scratch = x_test_59/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "woTfNst_ynRG"
   },
   "source": [
    "### 4. In the model which was built above (for classification of classes 0-4 in CIFAR10), make only the dense layers to be trainable and conv layers to be non-trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_VCDB3Byb1a"
   },
   "outputs": [],
   "source": [
    "transf_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_model.set_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in transf_model.layers[:-3]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.convolutional.Conv2D object at 0xb275a6da0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0xb275a6f60> False\n",
      "<keras.layers.convolutional.Conv2D object at 0xb3433b3c8> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0xb2758acc0> False\n",
      "<keras.layers.core.Dropout object at 0xb3f455f28> False\n",
      "<keras.layers.convolutional.Conv2D object at 0xb3f455eb8> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0xb3f455ef0> False\n",
      "<keras.layers.core.Dropout object at 0xb3f37bbe0> False\n",
      "<keras.layers.pooling.GlobalAveragePooling2D object at 0xb3f37bd68> False\n",
      "<keras.layers.core.Dense object at 0xb3f3aa6d8> True\n",
      "<keras.layers.core.Dropout object at 0xb3f48d550> True\n",
      "<keras.layers.core.Dense object at 0xb3f4f1e10> True\n"
     ]
    }
   ],
   "source": [
    "for layer in transf_model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-uUPqWpyeyX"
   },
   "source": [
    "### 5. Utilize the the model trained on CIFAR 10 (classes 0 to 4) to classify the classes 5 to 9 of CIFAR 10  (Use Transfer Learning) <br>\n",
    "Achieve an accuracy of more than 85% on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szHjJgDvyfCt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "  256/20000 [..............................] - ETA: 10s - loss: 4.3165 - acc: 0.1133"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 12s 581us/step - loss: 1.0759 - acc: 0.5846 - val_loss: 0.7515 - val_acc: 0.7244\n",
      "Epoch 2/50\n",
      "20000/20000 [==============================] - 12s 608us/step - loss: 0.7712 - acc: 0.7079 - val_loss: 0.6081 - val_acc: 0.7746\n",
      "Epoch 3/50\n",
      "20000/20000 [==============================] - 13s 626us/step - loss: 0.6703 - acc: 0.7484 - val_loss: 0.5637 - val_acc: 0.7822\n",
      "Epoch 4/50\n",
      "20000/20000 [==============================] - 14s 680us/step - loss: 0.6076 - acc: 0.7727 - val_loss: 0.5138 - val_acc: 0.8178\n",
      "Epoch 5/50\n",
      "20000/20000 [==============================] - 13s 654us/step - loss: 0.5731 - acc: 0.7904 - val_loss: 0.4956 - val_acc: 0.8170\n",
      "Epoch 6/50\n",
      "20000/20000 [==============================] - 13s 648us/step - loss: 0.5372 - acc: 0.8045 - val_loss: 0.4745 - val_acc: 0.8218\n",
      "Epoch 7/50\n",
      "20000/20000 [==============================] - 13s 651us/step - loss: 0.5092 - acc: 0.8118 - val_loss: 0.4434 - val_acc: 0.8332\n",
      "Epoch 8/50\n",
      "20000/20000 [==============================] - 13s 651us/step - loss: 0.4827 - acc: 0.8252 - val_loss: 0.4489 - val_acc: 0.8342\n",
      "Epoch 9/50\n",
      "20000/20000 [==============================] - 13s 636us/step - loss: 0.4680 - acc: 0.8306 - val_loss: 0.4145 - val_acc: 0.8482\n",
      "Epoch 10/50\n",
      "20000/20000 [==============================] - 13s 655us/step - loss: 0.4524 - acc: 0.8337 - val_loss: 0.4065 - val_acc: 0.8506\n",
      "Epoch 11/50\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.4316 - acc: 0.8391 - val_loss: 0.3946 - val_acc: 0.8598\n",
      "Epoch 12/50\n",
      "20000/20000 [==============================] - 13s 632us/step - loss: 0.4196 - acc: 0.8449 - val_loss: 0.3872 - val_acc: 0.8556\n",
      "Epoch 13/50\n",
      "20000/20000 [==============================] - 13s 654us/step - loss: 0.3974 - acc: 0.8509 - val_loss: 0.3872 - val_acc: 0.8568\n",
      "Epoch 14/50\n",
      "20000/20000 [==============================] - 13s 637us/step - loss: 0.3942 - acc: 0.8562 - val_loss: 0.3720 - val_acc: 0.8606\n",
      "Epoch 15/50\n",
      "20000/20000 [==============================] - 13s 653us/step - loss: 0.3786 - acc: 0.8606 - val_loss: 0.3558 - val_acc: 0.8668\n",
      "Epoch 16/50\n",
      "20000/20000 [==============================] - 13s 636us/step - loss: 0.3742 - acc: 0.8638 - val_loss: 0.3686 - val_acc: 0.8692\n",
      "Epoch 17/50\n",
      "20000/20000 [==============================] - 13s 657us/step - loss: 0.3709 - acc: 0.8632 - val_loss: 0.3874 - val_acc: 0.8556\n",
      "Epoch 18/50\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.3538 - acc: 0.8687 - val_loss: 0.3672 - val_acc: 0.8596\n",
      "Epoch 19/50\n",
      "20000/20000 [==============================] - 13s 673us/step - loss: 0.3484 - acc: 0.8722 - val_loss: 0.3914 - val_acc: 0.8610\n",
      "Epoch 20/50\n",
      "20000/20000 [==============================] - 14s 703us/step - loss: 0.3409 - acc: 0.8746 - val_loss: 0.3515 - val_acc: 0.8710\n",
      "Epoch 21/50\n",
      "20000/20000 [==============================] - 13s 671us/step - loss: 0.3363 - acc: 0.8767 - val_loss: 0.3515 - val_acc: 0.8686\n",
      "Epoch 22/50\n",
      "20000/20000 [==============================] - 13s 665us/step - loss: 0.3305 - acc: 0.8785 - val_loss: 0.3573 - val_acc: 0.8708\n",
      "Epoch 23/50\n",
      "20000/20000 [==============================] - 13s 665us/step - loss: 0.3250 - acc: 0.8819 - val_loss: 0.3394 - val_acc: 0.8746\n",
      "Epoch 24/50\n",
      "20000/20000 [==============================] - 13s 663us/step - loss: 0.3215 - acc: 0.8807 - val_loss: 0.3513 - val_acc: 0.8734\n",
      "Epoch 25/50\n",
      "20000/20000 [==============================] - 13s 663us/step - loss: 0.3123 - acc: 0.8839 - val_loss: 0.3387 - val_acc: 0.8772\n",
      "Epoch 26/50\n",
      "20000/20000 [==============================] - 14s 684us/step - loss: 0.2994 - acc: 0.8890 - val_loss: 0.3366 - val_acc: 0.8812\n",
      "Epoch 27/50\n",
      "20000/20000 [==============================] - 14s 694us/step - loss: 0.3055 - acc: 0.8871 - val_loss: 0.3385 - val_acc: 0.8762\n",
      "Epoch 28/50\n",
      "20000/20000 [==============================] - 13s 673us/step - loss: 0.3009 - acc: 0.8896 - val_loss: 0.3737 - val_acc: 0.8590\n",
      "Epoch 29/50\n",
      "20000/20000 [==============================] - 14s 695us/step - loss: 0.2938 - acc: 0.8916 - val_loss: 0.3397 - val_acc: 0.8784\n",
      "Epoch 30/50\n",
      "20000/20000 [==============================] - 14s 711us/step - loss: 0.2960 - acc: 0.8909 - val_loss: 0.3288 - val_acc: 0.8866\n",
      "Epoch 31/50\n",
      "20000/20000 [==============================] - 16s 799us/step - loss: 0.2849 - acc: 0.8965 - val_loss: 0.3384 - val_acc: 0.8798\n",
      "Epoch 32/50\n",
      "20000/20000 [==============================] - 16s 812us/step - loss: 0.2810 - acc: 0.8976 - val_loss: 0.3323 - val_acc: 0.8774\n",
      "Epoch 33/50\n",
      "20000/20000 [==============================] - 15s 727us/step - loss: 0.2781 - acc: 0.8982 - val_loss: 0.3586 - val_acc: 0.8708\n",
      "Epoch 34/50\n",
      "20000/20000 [==============================] - 15s 762us/step - loss: 0.2783 - acc: 0.8976 - val_loss: 0.3244 - val_acc: 0.8800\n",
      "Epoch 35/50\n",
      "20000/20000 [==============================] - 15s 737us/step - loss: 0.2719 - acc: 0.8996 - val_loss: 0.3293 - val_acc: 0.8802\n",
      "Epoch 36/50\n",
      "20000/20000 [==============================] - 15s 759us/step - loss: 0.2697 - acc: 0.8996 - val_loss: 0.3414 - val_acc: 0.8816\n",
      "Epoch 37/50\n",
      "20000/20000 [==============================] - 16s 821us/step - loss: 0.2713 - acc: 0.9020 - val_loss: 0.3407 - val_acc: 0.8734\n",
      "Epoch 38/50\n",
      "20000/20000 [==============================] - 15s 731us/step - loss: 0.2634 - acc: 0.9051 - val_loss: 0.3617 - val_acc: 0.8692\n",
      "Epoch 39/50\n",
      "20000/20000 [==============================] - 14s 723us/step - loss: 0.2583 - acc: 0.9049 - val_loss: 0.3102 - val_acc: 0.8862\n",
      "Epoch 40/50\n",
      "20000/20000 [==============================] - 15s 727us/step - loss: 0.2589 - acc: 0.9043 - val_loss: 0.3341 - val_acc: 0.8800\n",
      "Epoch 41/50\n",
      "20000/20000 [==============================] - 14s 719us/step - loss: 0.2547 - acc: 0.9071 - val_loss: 0.3503 - val_acc: 0.8778\n",
      "Epoch 42/50\n",
      "20000/20000 [==============================] - 14s 712us/step - loss: 0.2570 - acc: 0.9055 - val_loss: 0.3487 - val_acc: 0.8756\n",
      "Epoch 43/50\n",
      "20000/20000 [==============================] - 14s 717us/step - loss: 0.2522 - acc: 0.9057 - val_loss: 0.3387 - val_acc: 0.8842\n",
      "Epoch 44/50\n",
      "20000/20000 [==============================] - 14s 711us/step - loss: 0.2439 - acc: 0.9130 - val_loss: 0.3330 - val_acc: 0.8786\n",
      "Epoch 45/50\n",
      "20000/20000 [==============================] - 15s 730us/step - loss: 0.2476 - acc: 0.9079 - val_loss: 0.3105 - val_acc: 0.8890\n",
      "Epoch 46/50\n",
      "20000/20000 [==============================] - 14s 714us/step - loss: 0.2449 - acc: 0.9085 - val_loss: 0.3491 - val_acc: 0.8744\n",
      "Epoch 47/50\n",
      "20000/20000 [==============================] - 16s 778us/step - loss: 0.2428 - acc: 0.9130 - val_loss: 0.3326 - val_acc: 0.8812\n",
      "Epoch 48/50\n",
      "20000/20000 [==============================] - 18s 913us/step - loss: 0.2491 - acc: 0.9071 - val_loss: 0.3282 - val_acc: 0.8836\n",
      "Epoch 49/50\n",
      "20000/20000 [==============================] - 15s 762us/step - loss: 0.2335 - acc: 0.9140 - val_loss: 0.3336 - val_acc: 0.8784\n",
      "Epoch 50/50\n",
      "20000/20000 [==============================] - 15s 743us/step - loss: 0.2339 - acc: 0.9157 - val_loss: 0.3071 - val_acc: 0.8924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb76bf5198>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transf_model.fit(X_train_59_scratch, y_train_59_one_hot, batch_size=32, epochs=50, \n",
    "          verbose=1, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 151us/step\n",
      "Accuracy on the Test Images:  0.8852\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model on the test data\n",
    "score = transf_model.evaluate(X_test_59_scratch, y_test_59_one_hot)\n",
    "\n",
    "#Accuracy on test data\n",
    "print('Accuracy on the Test Images: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0zDuRecXzEtr"
   },
   "source": [
    "# Text classification using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMPlEJhHzb6P"
   },
   "source": [
    "### 6. Load the dataset from sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fe-B59u3zHNb"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRrMemVQzbHU"
   },
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-sZX0UbJzmg5"
   },
   "source": [
    "### 7. Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CITr_5aXziJ2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xcESc5QXzr6p"
   },
   "source": [
    "### 8. Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ysInblUMzpvl"
   },
   "outputs": [],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DriL2yZ50DQq"
   },
   "source": [
    "###  a.  You can access the values for the target variable using .target attribute \n",
    "###  b. You can access the name of the class in the target variable with .target_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vlUuai99z1hX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEKzaDfSz5E-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clBMKHzC0_N1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: sd345@city.ac.uk (Michael Collier)\\nSubject: Converting images to HP LaserJet III?\\nNntp-Posting-Host: hampton\\nOrganization: The City University\\nLines: 14\\n\\nDoes anyone know of a good way (standard PC application/PD utility) to\\nconvert tif/img/tga files into LaserJet III format.  We would also like to\\ndo the same, converting to HPGL (HP plotter) files.\\n\\nPlease email any response.\\n\\nIs this the correct group?\\n\\nThanks in advance.  Michael.\\n-- \\nMichael Collier (Programmer)                 The Computer Unit,\\nEmail: M.P.Collier@uk.ac.city                The City University,\\nTel: 071 477-8000 x3769                      London,\\nFax: 071 477-8565                            EC1V 0HB.\\n',\n",
       " \"From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\nSubject: help: Splitting a trimming region along a mesh \\nOrganization: University Of Kentucky, Dept. of Math Sciences\\nLines: 28\\n\\n\\n\\n\\tHi,\\n\\n\\tI have a problem, I hope some of the 'gurus' can help me solve.\\n\\n\\tBackground of the problem:\\n\\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \\n\\tmapping of a 3d Bezier patch into 2d. The area in this domain\\n\\twhich is inside a trimming loop had to be rendered. The trimming\\n\\tloop is a set of 2d Bezier curve segments.\\n\\tFor the sake of notation: the mesh is made up of cells.\\n\\n\\tMy problem is this :\\n\\tThe trimming area has to be split up into individual smaller\\n\\tcells bounded by the trimming curve segments. If a cell\\n\\tis wholly inside the area...then it is output as a whole ,\\n\\telse it is trivially rejected. \\n\\n\\tDoes any body know how thiss can be done, or is there any algo. \\n\\tsomewhere for doing this.\\n\\n\\tAny help would be appreciated.\\n\\n\\tThanks, \\n\\tAni.\\n-- \\nTo get irritated is human, to stay cool, divine.\\n\",\n",
       " \"From: djohnson@cs.ucsd.edu (Darin Johnson)\\nSubject: Re: harrassed at work, could use some prayers\\nOrganization: =CSE Dept., U.C. San Diego\\nLines: 63\\n\\n(Well, I'll email also, but this may apply to other people, so\\nI'll post also.)\\n\\n>I've been working at this company for eight years in various\\n>engineering jobs.  I'm female.  Yesterday I counted and realized that\\n>on seven different occasions I've been sexually harrassed at this\\n>company.\\n\\n>I dreaded coming back to work today.  What if my boss comes in to ask\\n>me some kind of question...\\n\\nYour boss should be the person bring these problems to.  If he/she\\ndoes not seem to take any action, keep going up higher and higher.\\nSexual harrassment does not need to be tolerated, and it can be an\\nenormous emotional support to discuss this with someone and know that\\nthey are trying to do something about it.  If you feel you can not\\ndiscuss this with your boss, perhaps your company has a personnel\\ndepartment that can work for you while preserving your privacy.  Most\\ncompanies will want to deal with this problem because constant anxiety\\ndoes seriously affect how effectively employees do their jobs.\\n\\nIt is unclear from your letter if you have done this or not.  It is\\nnot inconceivable that management remains ignorant of employee\\nproblems/strife even after eight years (it's a miracle if they do\\nnotice).  Perhaps your manager did not bring to the attention of\\nhigher ups?  If the company indeed does seem to want to ignore the\\nentire problem, there may be a state agency willing to fight with\\nyou.  (check with a lawyer, a women's resource center, etc to find out)\\n\\nYou may also want to discuss this with your paster, priest, husband,\\netc.  That is, someone you know will not be judgemental and that is\\nsupportive, comforting, etc.  This will bring a lot of healing.\\n\\n>So I returned at 11:25, only to find that ever single\\n>person had already left for lunch.  They left at 11:15 or so.  No one\\n>could be bothered to call me at the other building, even though my\\n>number was posted.\\n\\nThis happens to a lot of people.  Honest.  I believe it may seem\\nto be due to gross insensitivity because of the feelings you are\\ngoing through.  People in offices tend to be more insensitive while\\nworking than they normally are (maybe it's the hustle or stress or...)\\nI've had this happen to me a lot, often because they didn't realize\\nmy car was broken, etc.  Then they will come back and wonder why I\\ndidn't want to go (this would tend to make me stop being angry at\\nbeing ignored and make me laugh).  Once, we went off without our\\nboss, who was paying for the lunch :-)\\n\\n>For this\\n>reason I hope good Mr. Moderator allows me this latest indulgence.\\n\\nWell, if you can't turn to the computer for support, what would\\nwe do?  (signs of the computer age :-)\\n\\nIn closing, please don't let the hateful actions of a single person\\nharm you.  They are doing it because they are still the playground\\nbully and enjoy seeing the hurt they cause.  And you should not\\naccept the opinions of an imbecile that you are worthless - much\\nwiser people hold you in great esteem.\\n-- \\nDarin Johnson\\ndjohnson@ucsd.edu\\n  - Luxury!  In MY day, we had to make do with 5 bytes of swap...\\n\",\n",
       " 'From: s0612596@let.rug.nl (M.M. Zwart)\\nSubject: catholic church poland\\nOrganization: Faculteit der Letteren, Rijksuniversiteit Groningen, NL\\nLines: 10\\n\\nHello,\\n\\nI\\'m writing a paper on the role of the catholic church in Poland after 1989. \\nCan anyone tell me more about this, or fill me in on recent books/articles(\\nin english, german or french). Most important for me is the role of the \\nchurch concerning the abortion-law, religious education at schools,\\nbirth-control and the relation church-state(government). Thanx,\\n\\n                                                 Masja,\\n\"M.M.Zwart\"<s0612596@let.rug.nl>\\n',\n",
       " 'From: stanly@grok11.columbiasc.ncr.com (stanly)\\nSubject: Re: Elder Brother\\nOrganization: NCR Corp., Columbia SC\\nLines: 15\\n\\nIn article <Apr.8.00.57.41.1993.28246@athos.rutgers.edu> REXLEX@fnal.gov writes:\\n>In article <Apr.7.01.56.56.1993.22824@athos.rutgers.edu> shrum@hpfcso.fc.hp.com\\n>Matt. 22:9-14 \\'Go therefore to the main highways, and as many as you find\\n>there, invite to the wedding feast.\\'...\\n\\n>hmmmmmm.  Sounds like your theology and Christ\\'s are at odds. Which one am I \\n>to believe?\\n\\nIn this parable, Jesus tells the parable of the wedding feast. \"The kingdom\\nof heaven is like unto a certain king which made a marriage for his son\".\\nSo the wedding clothes were customary,  and \"given\" to those who \"chose\" to\\nattend.  This man \"refused\" to wear the clothes.  The wedding clothes are\\nequalivant to the \"clothes of righteousness\".  When Jesus died for our sins,\\nthose \"clothes\" were then provided.  Like that man, it is our decision to\\nput the clothes on.\\n']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTz4EaN_1WGc"
   },
   "source": [
    "### 9.  Now with dependent and independent data available for both train and test datasets, using TfidfVectorizer fit and transform the training data and test data and get the tfidf features for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H5G477f81C0Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.5 s, sys: 16.8 ms, total: 15.5 s\n",
      "Wall time: 15.6 s\n",
      "CPU times: user 11.3 s, sys: 8.99 ms, total: 11.3 s\n",
      "Wall time: 11.3 s\n",
      "(2257, 980)\n",
      "(1502, 980)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer1 = TfidfVectorizer(tokenizer=tokenize_and_stem, stop_words='english', max_features=980)\n",
    "tfidf_vectorizer2 = TfidfVectorizer(tokenizer=tokenize_and_stem, stop_words='english', max_features=980)\n",
    "\n",
    "%time tfidf_train_matrix = tfidf_vectorizer1.fit_transform(twenty_train.data) #fit the vectorizer to synopses\n",
    "%time tfidf_test_matrix = tfidf_vectorizer2.fit_transform(twenty_test.data) #fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_train_matrix.shape)\n",
    "print(tfidf_test_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tp_fDINJ1t4L"
   },
   "source": [
    "### 10. Use logisticRegression with tfidf features as input and targets as output and train the model and report the train and test accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THlN2b5d1yQp"
   },
   "outputs": [],
   "source": [
    "logi_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi_model.fit(tfidf_train_matrix, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.973859105006646"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi_model.score(tfidf_train_matrix, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4786950732356858"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi_model.score(tfidf_test_matrix, twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "R8_External_Lab_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
